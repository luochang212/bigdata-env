# é¡¹ç›®æ–‡æ¡£

## ğŸ’» ä½¿ç”¨è¯´æ˜

### Spark ä»»åŠ¡æäº¤
```bash
# è¿›å…¥ä¸»èŠ‚ç‚¹å®¹å™¨
docker exec -it spark-hadoop-hive-master bash

# æäº¤ Spark ä»»åŠ¡
spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://master:7077 \
  --deploy-mode client \
  $SPARK_HOME/examples/jars/spark-examples_2.12-3.3.0.jar \
  10
```

### HDFS æ“ä½œ
```bash
# æŸ¥çœ‹ HDFS æ–‡ä»¶ç³»ç»Ÿ
hdfs dfs -ls /

# åˆ›å»ºç›®å½•
hdfs dfs -mkdir /user/data

# ä¸Šä¼ æ–‡ä»¶
hdfs dfs -put /opt/share/sample.txt /user/data/

# ä¸‹è½½æ–‡ä»¶
hdfs dfs -get /user/data/sample.txt /opt/share/
```

### Hive æ•°æ®æ“ä½œ
```bash
# å¯åŠ¨ Hive CLI
hive

# åˆ›å»ºæ•°æ®åº“
CREATE DATABASE IF NOT EXISTS test_db;

# ä½¿ç”¨æ•°æ®åº“
USE test_db;

# åˆ›å»ºè¡¨
CREATE TABLE IF NOT EXISTS employees (
  id INT,
  name STRING,
  department STRING
) STORED AS TEXTFILE;

# æ’å…¥æ•°æ®
INSERT INTO employees VALUES (1, 'John', 'IT'), (2, 'Jane', 'HR');

# æŸ¥è¯¢æ•°æ®
SELECT * FROM employees;
```

### JupyterLab ä¸­ä½¿ç”¨ PySpark
```python
from pyspark.sql import SparkSession

# åˆ›å»º Spark ä¼šè¯
spark = SparkSession.builder \
    .appName("BigDataEnv") \
    .master("spark://master:7077") \
    .getOrCreate()

# åˆ›å»º DataFrame
data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
columns = ["name", "age"]
df = spark.createDataFrame(data, columns)

# æ˜¾ç¤ºæ•°æ®
df.show()

# åœæ­¢ Spark ä¼šè¯
spark.stop()
```

## ğŸ”§ å¸¸ç”¨å‘½ä»¤

### æœåŠ¡ç®¡ç†
```bash
# å¯åŠ¨æœåŠ¡
docker compose up -d

# åœæ­¢æœåŠ¡
docker compose down

# é‡å¯æœåŠ¡
docker compose restart

# æŸ¥çœ‹æ—¥å¿—
docker compose logs -f [service_name]

# è¿›å…¥å®¹å™¨
docker exec -it spark-hadoop-hive-master bash
```

### é›†ç¾¤ç›‘æ§
```bash
# æŸ¥çœ‹ Spark é›†ç¾¤çŠ¶æ€
curl http://localhost:8080/json/

# æŸ¥çœ‹ HDFS çŠ¶æ€
hdfs dfsadmin -report

# æŸ¥çœ‹ YARN åº”ç”¨
yarn application -list
```

### æ•°æ®ç®¡ç†
```bash
# å¤‡ä»½ HDFS æ•°æ®
hdfs dfs -cp /user/data /backup/data

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
hdfs dfs -rm -r /tmp/*

# æ£€æŸ¥ HDFS å¥åº·çŠ¶æ€
hdfs fsck /
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. å®¹å™¨å¯åŠ¨å¤±è´¥
```bash
# æ£€æŸ¥å®¹å™¨çŠ¶æ€
docker compose ps

# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
docker compose logs [service-name]

# æ£€æŸ¥èµ„æºä½¿ç”¨æƒ…å†µ
docker stats

# é‡æ–°æ„å»ºé•œåƒ
docker compose build --no-cache
```

#### 2. ç«¯å£å†²çª
å¦‚æœé‡åˆ°ç«¯å£è¢«å ç”¨çš„é”™è¯¯ï¼Œå¯ä»¥ä¿®æ”¹ `docker-compose.yml` ä¸­çš„ç«¯å£æ˜ å°„ï¼š
```yaml
ports:
  - '8081:8080'  # å°†ä¸»æœºç«¯å£æ”¹ä¸º 8081
```

#### 3. å†…å­˜ä¸è¶³
ç¡®ä¿ Docker åˆ†é…äº†è¶³å¤Ÿçš„å†…å­˜èµ„æºï¼ˆå»ºè®®è‡³å°‘ 8GBï¼‰ï¼š
- Docker Desktop: Settings â†’ Resources â†’ Memory

#### 4. Spark ä»»åŠ¡å¤±è´¥
- æ£€æŸ¥ Spark Master æ˜¯å¦æ­£å¸¸è¿è¡Œ
- ç¡®è®¤ Worker èŠ‚ç‚¹å·²è¿æ¥åˆ° Master
- æŸ¥çœ‹åº”ç”¨æ—¥å¿—ï¼šhttp://localhost:4040

#### 5. HDFS è¿æ¥é—®é¢˜
```bash
# æ£€æŸ¥ NameNode çŠ¶æ€
hdfs dfsadmin -report

# é‡æ–°æ ¼å¼åŒ– NameNodeï¼ˆè°¨æ…ä½¿ç”¨ï¼‰
hdfs namenode -format
```

#### 6. Hive è¿æ¥å¤±è´¥
```bash
# æ£€æŸ¥ MySQL è¿æ¥
docker exec -it hive-mysql mysql -u hive -phive123 -e "SHOW DATABASES;"

# é‡æ–°åˆå§‹åŒ– Hive å…ƒæ•°æ®
docker exec -it spark-hadoop-hive-master schematool -dbType mysql -initSchema
```

#### 7. JupyterLab æ— æ³•è®¿é—®
```bash
# æ£€æŸ¥ JupyterLab æ˜¯å¦æ­£åœ¨è¿è¡Œ
docker exec -it spark-hadoop-hive-master ps aux | grep jupyter

# é‡æ–°å¯åŠ¨ JupyterLab
docker exec -it spark-hadoop-hive-master /opt/miniconda3/bin/jupyter lab --allow-root --ip=0.0.0.0 --port=8888 --no-browser
```

### æ¸…ç†å’Œé‡ç½®

#### å®Œå…¨é‡ç½®ç¯å¢ƒ
```bash
# åœæ­¢æ‰€æœ‰æœåŠ¡
docker compose down

# åˆ é™¤æ‰€æœ‰å®¹å™¨å’Œå·
docker compose down -v

# åˆ é™¤é•œåƒï¼ˆå¯é€‰ï¼‰
docker rmi spark-hadoop-hive:latest

# é‡æ–°æ„å»ºå’Œå¯åŠ¨
docker compose build --no-cache
docker compose up -d
```

#### æ¸…ç† HDFS æ•°æ®
```bash
# è¿›å…¥ master å®¹å™¨
docker exec -it spark-hadoop-hive-master bash

# æ ¼å¼åŒ– NameNodeï¼ˆè°¨æ…æ“ä½œï¼Œä¼šåˆ é™¤æ‰€æœ‰ HDFS æ•°æ®ï¼‰
hdfs namenode -format
```

### æ€§èƒ½ä¼˜åŒ–

#### å†…å­˜é…ç½®
ç¼–è¾‘ `docker-compose.yml` è°ƒæ•´å†…å­˜åˆ†é…ï¼š
```yaml
environment:
  - SPARK_WORKER_MEMORY=2G  # å¢åŠ  Worker å†…å­˜
  - SPARK_WORKER_CORES=2    # å¢åŠ  CPU æ ¸å¿ƒæ•°
```

#### å­˜å‚¨ä¼˜åŒ–
```bash
# è®¾ç½® HDFS å‰¯æœ¬æ•°
hdfs dfsadmin -setDefaultReplication 2

# æ¸…ç† HDFS åƒåœ¾æ–‡ä»¶
hdfs dfs -expunge
```