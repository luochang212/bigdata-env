{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad8906c-5e20-440c-a25a-613e96b9af5f",
   "metadata": {},
   "source": [
    "# Spark 简单使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e7bf36a-8ad8-41f8-a49b-1d9e49c7cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4264e874-ad27-4080-b079-fac600ce2d9b",
   "metadata": {},
   "source": [
    "## 一、部署模式\n",
    "\n",
    "### 1.1 Local 模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a0121c-42d1-408a-8e75-ecb962a7832f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/10/02 05:35:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"jupyter-pyspark-demo\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab11a29a-b51b-48a6-bbec-791f9379b07e",
   "metadata": {},
   "source": [
    "### 1.2 Standalone 模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08372fd0-ea60-411f-bc8a-8cedac476a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/10/02 05:36:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .appName(\"jupyter-pyspark-standalone\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242b4248-6944-4260-a229-d36a3cccbaa1",
   "metadata": {},
   "source": [
    "### 1.3 YARN 模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ac252f-0730-4d09-a0f5-98a0298c6961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/10/02 05:36:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"jupyter-pyspark-yarn\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b7bcca-06f9-4ff0-a507-8a1bce19d2f6",
   "metadata": {},
   "source": [
    "## 二、文件系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704c7938-3dc7-4d9c-b2d4-b7592f00f77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/10/02 05:36:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"jupyter-pyspark-demo\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d66da63-5395-4cf6-82f2-160225107abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印统计信息\n",
    "def show_stats(df):\n",
    "    # 显示数据基本信息\n",
    "    print(\"数据集行数:\", df.count())\n",
    "    print(\"数据集列数:\", len(df.columns))\n",
    "    # print(\"\\n数据集结构:\")\n",
    "    # df.printSchema()\n",
    "    # print(\"\\n前5行数据:\")\n",
    "    # df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e302a-8683-4f6a-99a6-0bafb0b23308",
   "metadata": {},
   "source": [
    "### 2.1 本地文件系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8910270-6427-4ea4-9c4d-db4143c9fcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 导入 CSV 文件 (使用本地文件系统路径)\n",
    "df_local = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file:///opt/share/spark_data/PRSA_data_2010.1.1-2014.12.31.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84b24b43-f44b-4bce-965d-7236258c0693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集行数: 43824\n",
      "数据集列数: 13\n"
     ]
    }
   ],
   "source": [
    "show_stats(df_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2442710-3a26-4f26-b936-e68f8bfd9f25",
   "metadata": {},
   "source": [
    "### 2.2 分布式文件系统"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b919b-f747-4261-a6aa-d05c1399eaf6",
   "metadata": {},
   "source": [
    "在使用 pyspark 读取 HDFS 文件之前，需要先把本地文件上传到 HDFS。\n",
    "\n",
    "```bash\n",
    "# 进入 bash\n",
    "# docker compose exec -it spark bash\n",
    "docker exec -it spark-hadoop-hive-master bash\n",
    "\n",
    "# 激活 conda 环境\n",
    "conda activate base\n",
    "\n",
    "# 查看 hdfs 集群状态\n",
    "hdfs dfsadmin -report\n",
    "\n",
    "# 创建 hdfs 路径\n",
    "hdfs dfs -mkdir -p /user/root\n",
    "\n",
    "# 将本地数据上传到 hdfs\n",
    "hdfs dfs -put /opt/share/spark_data/PRSA_data_2010.1.1-2014.12.31.csv /user/root/PRSA_data_2010.1.1-2014.12.31.csv\n",
    "\n",
    "# 检查上传是否成功\n",
    "hdfs dfs -ls /user/root/\n",
    "\n",
    "# 删除 HDFS 文件\n",
    "hdfs dfs -rm /user/root/PRSA_data_2010.1.1-2014.12.31.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f56b92a0-e63f-46f4-b87e-2504dc708527",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdfs = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs://master:9000/user/root/PRSA_data_2010.1.1-2014.12.31.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ada8e10a-6bd6-4be5-85df-39d5ba2cf64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集行数: 43824\n",
      "数据集列数: 13\n"
     ]
    }
   ],
   "source": [
    "show_stats(df_hdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c427c6-b20b-4760-86ab-91044e54effc",
   "metadata": {},
   "source": [
    "## 三、分布式检查\n",
    "\n",
    "检查文件是否分布式存储：\n",
    "\n",
    "```bash\n",
    "# 检查指定文件的健康状态\n",
    "hdfs fsck /PRSA_data_2010.1.1-2014.12.31.csv -files -blocks -locations\n",
    "\n",
    "# 获取 HDFS 集群配置参数\n",
    "hdfs getconf -confKey dfs.blocksize\n",
    "\n",
    "# 获取 HDFS 集群状态报告\n",
    "hdfs dfsadmin -report\n",
    "\n",
    "# 检查 yarn 服务的状态\n",
    "yarn node -list \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886001bb-4f81-4c25-a555-2cadcd13287e",
   "metadata": {},
   "source": [
    "## 四、使用 `spark-submit` 提交 Python 任务\n",
    "\n",
    "脚本已在 `spark.py` 和 `spark-submit.sh` 中写好。\n",
    "\n",
    "运行以下代码提交任务：\n",
    "\n",
    "```bash\n",
    "bash spark-submit.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76b2fb-c3ad-4d8f-89ec-5ab55500b16f",
   "metadata": {},
   "source": [
    "## 五、使用 `hive -f` 提交 Hive 任务\n",
    "\n",
    "两种用法：\n",
    "- `hive -f [文件名]`\n",
    "- `hive -e [SQL代码]`\n",
    "\n",
    "脚本已在 `hive-submit.sh` 中写好。\n",
    "\n",
    "运行以下代码提交任务：\n",
    "\n",
    "```bash\n",
    "bash hive-submit.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaebc34c-a0d0-4496-b469-44cc9d1941a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
